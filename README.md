# JobFit AI

## Live application Links :octopus:


[![Demo Link](https://img.shields.io/badge/Demo_Link-808080?style=for-the-badge&logo=YouTube&logoColor=white)](https://youtu.be/UyE7B184TPE)

[![Codelab](https://img.shields.io/badge/Codelab-4B4BFF?style=for-the-badge&logo=Codelab&logoColor=white)](https://codelabs-preview.appspot.com/?file_id=1vgMyIUyW9-KYcdxPMUs-lFjF9gVwKMT2BpwPWluPthc/edit#0)



Streamlit Login:
username: aniket
password: aniket

Resume and Job description for demo: https://drive.google.com/drive/folders/1m1yMmF6372qY8z2OD7hg2uU0jsa9c6r3?usp=sharing

## Abstract :memo: 

The "JobFit AI" project introduces an innovative AI-driven Applicant Tracking System (ATS) aimed at optimizing the resume tailoring process. It facilitates seamless communication between users and the application through a Streamlit frontend and FastAPI backend. By leveraging cutting-edge technologies such as OpenAI's GPT, the system offers personalized suggestions for resume enhancement, empowering users to tailor their resumes to specific job requirements effectively.

## Problem Statement :construction:
### Current Challenges
Many students face significant challenges in efficiently screening job descriptions, analyzing the skills required, comparing with their resumes and updating their resumes and gaining expertise in skills they are lagging behind during the application process. Some common challenges include:
1. Limited Guidance: Many students or applicants lack guidance on how to optimize their resumes for specific job roles or industries, leading to missed opportunities in the job market.
2. Time-Consuming Process: Manual resume optimization can be time-consuming and labor-intensive, especially for individuals with limited experience or resources.
3. Competitive Job Market: In a competitive job market, it's essential for resumes to stand out and effectively showcase candidates' qualifications and skills.
4. Lack of Personalization: Generic resume templates and advice may not adequately address individual strengths, experiences, and career goals.

## Project Goals :dart:

1. Leverage Pydantic or NLP to extract key skills and experience from resumes.
2. Compare extracted data with job requirements and provide personalized improvement suggestions.
3. Utilize user data and APIs to recommend relevant job openings.
4. Tailor resumes to match job descriptions through keyword analysis and suggestion generation.
5. Integrate Chat GPT (or similar) for enhanced and refined resume improvement recommendations.
6. Develop a user-friendly Streamlit application for a seamless user experience.

## Use case :bookmark_tabs:

Job seekers struggling to tailor their resumes for specific job roles can leverage JobFit AI. By uploading their resume and a desired job description, JobFit AI analyzes both documents. It  identifies missing skills, suggests targeted improvements, and recommends relevant job openings based on the user's experience. This empowers job seekers to present strong resumes and increase their chances of landing their dream job.

## Technologies Used :computer:

[![Apache Airflow](https://img.shields.io/badge/Apache%20Airflow-2.9.0-017CEE?style=for-the-badge&logo=Apache%20Airflow&logoColor=white)](https://airflow.apache.org/)
[![Beautiful Soup](https://img.shields.io/badge/Beautiful%20Soup-4.12.3-3776AB?style=for-the-badge&logo=Python&logoColor=white)](https://pypi.org/project/beautifulsoup4/)
[![Boto3](https://img.shields.io/badge/Boto3-1.34.74-FF9900?style=for-the-badge&logo=Amazon%20AWS&logoColor=white)](https://github.com/boto/boto3)
[![Botocore](https://img.shields.io/badge/Botocore-1.34.74-FF9900?style=for-the-badge&logo=Amazon%20AWS&logoColor=white)](https://github.com/boto/botocore)
[![Diagrams](https://img.shields.io/badge/Diagrams-0.23.4-009688?style=for-the-badge&logo=Python&logoColor=white)](https://diagrams.mingrammer.com/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.110.2-009688?style=for-the-badge&logo=FastAPI&logoColor=white)](https://fastapi.tiangolo.com/)
[![Jobspy](https://img.shields.io/badge/Jobspy-0.29.0-4CAF50?style=for-the-badge&logo=Python&logoColor=white)](https://pypi.org/project/jobspy/)
[![LinkedIn API](https://img.shields.io/badge/LinkedIn%20API-2.1.1-0077B5?style=for-the-badge&logo=LinkedIn&logoColor=white)](https://github.com/ozgur/python-linkedin)
[![NLTK](https://img.shields.io/badge/NLTK-3.8.1-FF6F00?style=for-the-badge&logo=Python&logoColor=white)](https://www.nltk.org/)
[![OpenAI](https://img.shields.io/badge/OpenAI-1.23.6-FF6F00?style=for-the-badge&logo=OpenAI&logoColor=white)](https://openai.com/)
[![Pandas](https://img.shields.io/badge/Pandas-2.2.2-2C2D72?style=for-the-badge&logo=pandas&logoColor=white)](https://pandas.pydata.org/)
[![Pinecone](https://img.shields.io/badge/Pinecone-0.1.0-4CAF50?style=for-the-badge&logo=Python&logoColor=white)](https://www.pinecone.io/)
[![Pydantic](https://img.shields.io/badge/Pydantic-2.7.1-009688?style=for-the-badge&logo=Python&logoColor=white)](https://pydantic-docs.helpmanual.io/)
[![Pydparser](https://img.shields.io/badge/Pydparser-1.0.4-4CAF50?style=for-the-badge&logo=Python&logoColor=white)](https://pypi.org/project/pydparser/)
[![PyMongo](https://img.shields.io/badge/PyMongo-3.13.0-47A248?style=for-the-badge&logo=Python&logoColor=white)](https://pymongo.readthedocs.io/en/stable/)
[![Python-Dotenv](https://img.shields.io/badge/Python%20Dotenv-1.0.1-4CAF50?style=for-the-badge&logo=Python&logoColor=white)](https://github.com/theskumar/python-dotenv)
[![PyTZ](https://img.shields.io/badge/PyTZ-2024.1-FF6F00?style=for-the-badge&logo=Python&logoColor=white)](https://pypi.org/project/pytz/)
[![PyYAML](https://img.shields.io/badge/PyYAML-6.0-FFD43B?style=for-the-badge&logo=Python&logoColor=white)](https://pyyaml.org/)
[![Requests](https://img.shields.io/badge/Requests-2.31.0-47A248?style=for-the-badge&logo=Python&logoColor=white)](https://docs.python-requests.org/en/master/)
[![Rich](https://img.shields.io/badge/Rich-13.7.1-009688?style=for-the-badge&logo=Python&logoColor=white)](https://rich.readthedocs.io/en/stable/)
[![Selenium](https://img.shields.io/badge/Selenium-4.20.0-43B02A?style=for-the-badge&logo=Selenium&logoColor=white)](https://www.selenium.dev/)
[![SkillNer](https://img.shields.io/badge/SkillNer-1.0.3-4CAF50?style=for-the-badge&logo=Python&logoColor=white)](https://pypi.org/project/skillNer/)
[![Spacy](https://img.shields.io/badge/Spacy-3.7.2-09A3D5?style=for-the-badge&logo=Python&logoColor=white)](https://spacy.io/)
[![SQLAlchemy](https://img.shields.io/badge/SQLAlchemy-1.4.52-FF4500?style=for-the-badge&logo=Python&logoColor=white)](https://www.sqlalchemy.org/)
[![Streamlit](https://img.shields.io/badge/Streamlit-1.33.0-FF4B4B?style=for-the-badge&logo=Streamlit&logoColor=white)](https://streamlit.io/)
[![Streamlit Authenticator](https://img.shields.io/badge/Streamlit%20Authenticator-0.3.2-FF4B4B?style=for-the-badge&logo=Streamlit&logoColor=white)](https://github.com/whitphx/streamlit-authenticator)

## Data Source :flashlight:

The data is sourced from job portals such as LinkedIn and Glassdoor to access the latest available job positions. Additionally, datasets from Kaggle and Hugging Face are utilized for keywords.

## Process Outline :
**1. Data Acquisition:** Users upload their resumes and desired job descriptions through a user-friendly interface.
**2. Data Processing:** Utilize NLP libraries to extract key skills and experience from resumes.
**3. Analysis and Recommendations:**
- Compare extracted data with pre-defined skills or job requirements.
- Recommend improvements to the resume based on the job description and missing skills.
- Leverage APIs (e.g., LinkedIn) to suggest relevant job postings.
**4. AI-powered Enhancement:** Integrate Chat GPT to generate insightful and personalized suggestions for further resume optimization.
**5. User Interface:** Develop a user-friendly Streamlit application to display extracted skills, recommended improvements, job recommendations, and AI-powered suggestions.

## Requirements :briefcase:
```
apache-airflow==2.9.0
beautifulsoup4==4.12.3
boto3==1.34.74
botocore==1.34.74
diagrams==0.23.4
en_core_web_sm==2.3.1
fastapi==0.110.2
jobspy==0.29.0
linkedin_api==2.1.1
nltk==3.8.1
openai==1.23.6
pandas==2.2.2
pinecone==0.1.0
pydantic==2.7.1
pydparser==1.0.4
pymongo==3.13.0
python-dotenv==1.0.1
pytz==2023.3
pytz==2024.1
PyYAML==6.0
PyYAML==6.0.1
Requests==2.31.0
rich==13.7.1
selenium==4.20.0
skillNer==1.0.3
spacy==3.7.2
SQLAlchemy==1.4.52
streamlit==1.33.0
streamlit_authenticator==0.3.2
```

## Project Folder Structure :

```
ðŸ“¦ Final-Project-Playground
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ README.md
â”œâ”€â”€ __pycache__
â”‚   â””â”€â”€ constants.cpython-311.pyc
â”œâ”€â”€ __test__
â”‚   â””â”€â”€ main.py
â”œâ”€â”€ architecture.py
â”œâ”€â”€ assets
â”‚   â”œâ”€â”€ csv.jpeg
â”‚   â”œâ”€â”€ linkedin_logo.png
â”‚   â”œâ”€â”€ openai.png
â”‚   â”œâ”€â”€ pinecone.png
â”‚   â”œâ”€â”€ streamlit.png
â”‚   â””â”€â”€ user.png
â”œâ”€â”€ backend
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ __pycache__
â”‚   â”‚   â”œâ”€â”€ constants.cpython-311.pyc
â”‚   â”‚   â”œâ”€â”€ database.cpython-311.pyc
â”‚   â”‚   â”œâ”€â”€ job_match_score.cpython-311.pyc
â”‚   â”‚   â”œâ”€â”€ pinecone_util.cpython-311.pyc
â”‚   â”‚   â”œâ”€â”€ resume_analyser.cpython-311.pyc
â”‚   â”‚   â””â”€â”€ utils.cpython-311.pyc
â”‚   â”œâ”€â”€ constants.py
â”‚   â”œâ”€â”€ database.py
â”‚   â”œâ”€â”€ datetime_manage.py
â”‚   â”œâ”€â”€ job_match_score.py
â”‚   â”œâ”€â”€ pinecone_util.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ resume_analyser.py
â”‚   â””â”€â”€ utils.py
â”œâ”€â”€ config
â”œâ”€â”€ constants.py
â”œâ”€â”€ dags
â”‚   â”œâ”€â”€ __pycache__
â”‚   â”‚   â””â”€â”€ workflow.cpython-312.pyc
â”‚   â”œâ”€â”€ tasks
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ __pycache__
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.cpython-312.pyc
â”‚   â”‚   â”‚   â””â”€â”€ scrape.cpython-312.pyc
â”‚   â”‚   â”œâ”€â”€ datetime_manage.py
â”‚   â”‚   â”œâ”€â”€ embeddings.py
â”‚   â”‚   â”œâ”€â”€ linkedIn.py
â”‚   â”‚   â”œâ”€â”€ linkedIn_util.py
â”‚   â”‚   â”œâ”€â”€ manage_mongo_db.py
â”‚   â”‚   â”œâ”€â”€ pydantic_models copy.py
â”‚   â”‚   â”œâ”€â”€ pydantic_models.py
â”‚   â”‚   â”œâ”€â”€ scrape.py
â”‚   â”‚   â””â”€â”€ util.py
â”‚   â””â”€â”€ workflow.py
â”œâ”€â”€ docker-compose.yaml
â”œâ”€â”€ frontend
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ __pycache__
â”‚   â”‚   â”œâ”€â”€ auth.cpython-311.pyc
â”‚   â”‚   â”œâ”€â”€ job_fit_score.cpython-311.pyc
â”‚   â”‚   â”œâ”€â”€ job_recomendation.cpython-311.pyc
â”‚   â”‚   â””â”€â”€ resume_analyser.cpython-311.pyc
â”‚   â”œâ”€â”€ auth.py
â”‚   â”œâ”€â”€ cred.yaml
â”‚   â”œâ”€â”€ job_fit_score.py
â”‚   â”œâ”€â”€ job_recomendation.py
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ resume_analyser.py
â”œâ”€â”€ main.py
â”œâ”€â”€ plugins
â”œâ”€â”€ project.ipynb
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ scripts
â”‚   â”œâ”€â”€ Embeddings.ipynb
â”‚   â”œâ”€â”€ JobSpy_Demo.ipynb
â”‚   â”œâ”€â”€ LinkedIn.ipynb
â”‚   â”œâ”€â”€ __pycache__
â”‚   â”‚   â”œâ”€â”€ skillExtracter.cpython-311.pyc
â”‚   â”‚   â””â”€â”€ util.cpython-311.pyc
â”‚   â”œâ”€â”€ build_knowledge_jd.ipynb
â”‚   â”œâ”€â”€ building_knowledge_resume.ipynb
â”‚   â”œâ”€â”€ job_scraping.py
â”‚   â”œâ”€â”€ project.ipynb
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ resume_embedding.csv
â”‚   â”œâ”€â”€ skillExtracter.py
â”‚   â””â”€â”€ util.py
â””â”€â”€ setup.sh
```

## How to run Application locally :rocket:

To run the application locally, follow these steps:

1. Clone the repository to get all the source code on your machine.

2. Create a virtual environment and install all requirements from the requirements.txt file present.

3. Create a .env file in the root directory with the following variables:

    OPENAI_API_KEY: your OpenAI API key

    PINECONE_API_KEY: your Pinecone API key

    AWS_ACCESS_KEY_ID: your AWS access ID

    AWS_SECRET_ACCESS_KEY: your AWS secret key

    S3_BUCKET_NAME: S3 bucket name

4. Once you have set up your environment variables, start Airflow and Application by running the following command

`docker-compose up airflow-init && docker-compose up -d`

5. Access the Airflow UI by navigating to `http://localhost:8024/` in your web browser.

6. To run the DAG in Airflow, click on the dag link on the Airflow UI and toggle the switch to enable the DAGs.

7. Once the DAGs have run successfully,

8. Access the Streamlit UI by navigating to `http://localhost:8501/` in your web browser.

## References :books:
- https://code.visualstudio.com/docs/python/tutorial-fastapiLinks
- https://aws.amazon.com/s3/
- https://github.com/Coding-Crashkurse/Pydantic-v2-crashcourseLinks to an external site.
- https://docs.pinecone.io/guides/getting-started/quickstart/Using_Pinecone_for_embeddings_search.ipynb
- https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/
- https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/GPT4_Retrieval_Augmentation.ipynb


## Learning Outcomes :
List the learning outcomes from the assignment/project
### Technical skills:
1. Proficiency in Natural Language Processing (NLP): Gain proficiency in leveraging OpenAI's GPT API for analyzing and processing text data from resumes and job descriptions.
2. Experience with Streamlit and FastAPI: Acquire familiarity with building interactive web applications using Streamlit for the frontend and FastAPI for the backend, including handling user input, authentication, and data communication.
3. Understanding of AI-driven Systems: Develop an understanding of how AI-driven systems can be integrated into applications to provide personalized recommendations and enhance user experiences.
4. Knowledge of API Integration: Learn how to integrate external APIs, such as the LinkedIn API, for fetching real-time job descriptions and enhancing the system's recommendation capabilities.
5. Data Management and Storage: Gain experience in managing and storing large datasets, including resumes and job descriptions, using technologies like Amazon S3 for data storage and retrieval.
### Softskills:
1. Problem-Solving Abilities: Discuss how the project challenged participants to think critically, solve problems, and overcome obstacles.
2. Project Management Skills: Highlight any experience gained in project planning, organization, task management, and collaboration within a team.
3. Communication and Collaboration: Reflect on the effectiveness of communication and collaboration within the team, including teamwork, leadership, and interpersonal skills.


## Team Information and Contribution :handshake:

Name            | Contribution % | Contribution                         |
---             | ---            | ---                                  |
Aniket Giram    | 33.33%         |Text, keywords extraction, Embeddings |
Sudarshan Dudhe | 33.33%         |Backend, pytest, testing              |
Rasika Kole     | 33.33%         |Frontend, Backend, Documentation      |
Detail tasks are added in codelab

## Contact Information

Aniket Giram: giram.a@northeastern.edu
Sudarshan Dudhe: dudhe.s@northeastern.edu
Rasika Kole: kole.r@northeastern.edu
