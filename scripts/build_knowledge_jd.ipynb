{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skillExtracter import extractKeywordFromLightSkillAPI\n",
    "import json\n",
    "from util import getAccessToken\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/job_descriptions/jobs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = json.loads(getAccessToken())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(1.1) == type(2.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# row.description is pd.isnull or row.description == None or pd.isna(row.description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "chunksize = 1000\n",
    "for chunk in pd.read_csv('data/job_descriptions/jobs.csv', chunksize=chunksize):\n",
    "    print(\"processing 1000 record\")\n",
    "    for row in chunk.itertuples(index=False):\n",
    "        if pd.isna(row.description):\n",
    "            continue\n",
    "        job = {}\n",
    "        job['description'] = row.description\n",
    "        job['Industries'] = row.Industries\n",
    "        job['company'] = row.company\n",
    "        job['context'] = row.context\n",
    "        job['education'] = row.education\n",
    "        job['location'] = row.location\n",
    "        job['months_experience'] = row.months_experience\n",
    "        job['sal_high'] = row.sal_high\n",
    "        job['sal_low'] = row.sal_low\n",
    "        job['salary'] = row.salary\n",
    "        job['title'] = row.title\n",
    "        skills = extractKeywordFromLightSkillAPI(row.description, token)     \n",
    "        job['skills'] = \",\".join(skills)\n",
    "        data.append(job)\n",
    "    print('processed 1000 data')\n",
    "\n",
    "newdf = pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf.to_csv(\"jd_with_keyword.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('jd_with_keyword.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pinecone-client openai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"unclosed\", category=ResourceWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL = \"text-embedding-3-small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv() \n",
    "client = openai.OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"None\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # old code updated code is at below sale with method name createEmbeddingWithValues\n",
    "# embeddings = []\n",
    "# chunksize = 1000\n",
    "# for chunk in pd.read_csv('jd_with_keyword.csv', chunksize=chunksize):\n",
    "#     print(\"processing 1000 record\")\n",
    "#     for row in chunk.itertuples(index=False):\n",
    "#         response = client.embeddings.create(model=EMBEDDING_MODEL, input=chunk)\n",
    "#         for i, be in enumerate(response.data):\n",
    "#             assert i == be.index\n",
    "#         batch_embeddings = [e.embedding for e in response.data]\n",
    "#         embeddings.extend(batch_embeddings)\n",
    "#     print(\"processed 1000 record\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createEmbeddingWithValues(data, extras):\n",
    "    result, __data =[], {}\n",
    "    response = client.embeddings.create(model=EMBEDDING_MODEL, input=data)\n",
    "    for i, be in enumerate(response.data):\n",
    "        assert i == be.index\n",
    "    batch_embeddings = [e.embedding for e in response.data]\n",
    "    __data[\"id\"] = str(extras['id'])\n",
    "    __data[\"values\"] = batch_embeddings[0]\n",
    "    __data[\"metadata\"] = {'extras': str(extras)}\n",
    "    result.append(__data)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertToDic(row):\n",
    "    dic = {}\n",
    "    dic[\"id\"] = str(row._0)\n",
    "    dic[\"description\"] = str(row.description)\n",
    "    dic[\"industries\"] = \"\" if pd.isnull(row.Industries) else str(row.Industries)\n",
    "    dic[\"company\"] = \"\" if pd.isnull(row.company) else str(row.company)\n",
    "    dic[\"education\"] = \"\" if pd.isnull(row.education) else str(row.education)\n",
    "    dic[\"months_experience\"] = 0 if pd.isna(row.months_experience) else row.months_experience\n",
    "    dic[\"sal_high\"] = 0 if pd.isna(row.sal_high) else row.sal_high\n",
    "    dic[\"sal_low\"] = 0 if pd.isna(row.sal_low) else row.sal_low\n",
    "    dic[\"salary\"] = 0 if pd.isna(row.salary) else row.salary\n",
    "    dic[\"title\"] = \"\" if pd.isnull(row.title) else str(row.title)\n",
    "    dic[\"skills\"] = \"\" if pd.isnull(row.skills) else str(row.skills)\n",
    "    return dic\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 500\n",
      "processing 500\n",
      "processed 500\n",
      "processing 500\n",
      "processed 500\n",
      "processing 500\n",
      "processed 500\n",
      "processing 500\n",
      "processed 500\n",
      "processing 500\n",
      "processed 500\n",
      "processing 500\n",
      "processed 500\n",
      "processing 500\n",
      "processed 500\n"
     ]
    }
   ],
   "source": [
    "description_embeddings = []\n",
    "skills_embeddings = []\n",
    "chunksize = 500\n",
    "for chunk in pd.read_csv('../data/job_descriptions/jd_with_keyword.csv', chunksize=chunksize):\n",
    "    print(\"processing 500\")\n",
    "    for row in chunk.itertuples(index=False):\n",
    "        if pd.isna(row.description) or pd.isna(row.skills):\n",
    "            continue\n",
    "        description_embeddings.extend(createEmbeddingWithValues(row.description, convertToDic(row)))\n",
    "        skills_embeddings.extend(createEmbeddingWithValues(row.skills, convertToDic(row)))\n",
    "    print(\"processed 500\")\n",
    "\n",
    "description_embeddings_df = pd.DataFrame(description_embeddings)\n",
    "skills_embeddings_df = pd.DataFrame(skills_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_embeddings_df.to_csv(\"../data/embeddings/description_embeddings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skills_embeddings_df.to_csv(\"../data/embeddings/skills_embeddings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_embeddings_df = pd.read_csv(\"../data/embeddings/description_embeddings.csv\")\n",
    "skills_embeddings_df = pd.read_csv(\"../data/embeddings/skills_embeddings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "432efbce-a192-4eb1-bba7-6b46fb18f1b5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aniketgiram/Desktop/Big-Data/JobFitAI/.venv/lib/python3.11/site-packages/pinecone/data/index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "_api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "print(_api_key)\n",
    "pc = Pinecone(\n",
    "    api_key=\"432efbce-a192-4eb1-bba7-6b46fb18f1b5\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_fit_index_name = 'job-fit'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Check whether the index with the same name already exists - if so, delete it\n",
    "if job_fit_index_name in pc.list_indexes():\n",
    "    pc.delete_index(job_fit_index_name)\n",
    "\n",
    "# Now do stuff\n",
    "if job_fit_index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name=job_fit_index_name,\n",
    "        dimension=1536,\n",
    "        metric='cosine',\n",
    "        spec=ServerlessSpec(\n",
    "            cloud='aws',\n",
    "            region='us-east-1'\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pc.Index(job_fit_index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.upsert(\n",
    "  vectors=description_embeddings,\n",
    "  namespace=\"description\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunker(seq, size):\n",
    "    \"\"\"Yields successive chunks from seq.\"\"\"\n",
    "    for pos in range(0, len(seq), size):\n",
    "        yield seq.iloc[pos:pos + size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data(chunk):\n",
    "    \"\"\"Converts a pandas DataFrame chunk to the format expected by Pinecone's upsert method.\"\"\"\n",
    "    data = []\n",
    "    for _, row in chunk.iterrows():\n",
    "        vector_id = str(row['id'])\n",
    "        embedding = row['values']\n",
    "        metadata = row.get('metadata', {})\n",
    "        if 'context' in eval(metadata['extras']):\n",
    "            extras = eval(metadata['extras'])\n",
    "            del extras['context']\n",
    "            metadata = extras\n",
    "        data.append((vector_id, embedding, metadata))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_embeddings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 100  # Define your chunk size\n",
    "for chunk in chunker(description_embeddings_df, chunk_size):\n",
    "    vectors_to_upsert = convert_data(chunk)\n",
    "    print(vectors_to_upsert)\n",
    "    print(\"vectorising chunk\")\n",
    "    index.upsert(\n",
    "        vectors=vectors_to_upsert,\n",
    "        namespace=\"description\",\n",
    "    )\n",
    "    print(\"upserted chunk\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorising chunk\n",
      "upserted chunk\n",
      "vectorising chunk\n",
      "upserted chunk\n",
      "vectorising chunk\n",
      "upserted chunk\n",
      "vectorising chunk\n",
      "upserted chunk\n",
      "vectorising chunk\n",
      "upserted chunk\n",
      "vectorising chunk\n",
      "upserted chunk\n",
      "vectorising chunk\n",
      "upserted chunk\n",
      "vectorising chunk\n",
      "upserted chunk\n",
      "vectorising chunk\n",
      "upserted chunk\n",
      "vectorising chunk\n",
      "upserted chunk\n",
      "vectorising chunk\n",
      "upserted chunk\n",
      "vectorising chunk\n",
      "upserted chunk\n",
      "vectorising chunk\n",
      "upserted chunk\n",
      "vectorising chunk\n",
      "upserted chunk\n",
      "vectorising chunk\n",
      "upserted chunk\n",
      "vectorising chunk\n",
      "upserted chunk\n",
      "vectorising chunk\n",
      "upserted chunk\n",
      "vectorising chunk\n",
      "upserted chunk\n",
      "vectorising chunk\n",
      "upserted chunk\n",
      "vectorising chunk\n",
      "upserted chunk\n",
      "vectorising chunk\n",
      "upserted chunk\n",
      "vectorising chunk\n",
      "upserted chunk\n",
      "vectorising chunk\n",
      "upserted chunk\n",
      "vectorising chunk\n",
      "upserted chunk\n",
      "vectorising chunk\n",
      "upserted chunk\n",
      "vectorising chunk\n",
      "upserted chunk\n",
      "vectorising chunk\n",
      "upserted chunk\n",
      "vectorising chunk\n",
      "upserted chunk\n",
      "vectorising chunk\n",
      "upserted chunk\n",
      "vectorising chunk\n",
      "upserted chunk\n",
      "vectorising chunk\n",
      "upserted chunk\n",
      "vectorising chunk\n",
      "upserted chunk\n",
      "vectorising chunk\n",
      "upserted chunk\n",
      "vectorising chunk\n",
      "upserted chunk\n",
      "vectorising chunk\n",
      "upserted chunk\n",
      "vectorising chunk\n",
      "upserted chunk\n",
      "vectorising chunk\n",
      "upserted chunk\n",
      "vectorising chunk\n",
      "upserted chunk\n",
      "vectorising chunk\n",
      "upserted chunk\n"
     ]
    }
   ],
   "source": [
    "chunk_size = 200  # Define your chunk size\n",
    "for chunk in chunker(skills_embeddings_df, chunk_size):\n",
    "    vectors_to_upsert = convert_data(chunk)\n",
    "    print(\"vectorising chunk\")\n",
    "    index.upsert(\n",
    "        vectors=vectors_to_upsert,\n",
    "        namespace=\"skills\"\n",
    "    )\n",
    "    print(\"upserted chunk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "Job Title: Senior Data Engineer Location: Alexandria, VA Salary Range: $120k - $150k Requirements: ETL/ELT, SQL, AWS/Google Cloud, Linux/Unix, Spark (preferred), NoSQL (preferred), Machine Learning concepts (preferred) Based in beautiful Alexandria, VA, we are one of the hottest media analytics and software start-ups in the DC area. Due to growth, we are actively seeking to hire a Senior Data Engineer to join our team. The ideal candidate will have at least 5 years of experience with data pipelines (built with Python), ETL experience, a strong SQL background, experience with AWS or Google Cloud, and strong Unix/Linux fundamentals. Any experience with Spark, NoSQL, and Machine Learning would be a huge plus. If this sounds like you, please apply now or send your resume to shiv.warrier@cybercoders.com! What You Will Be Doing Lead design and development of data pipelines Deliver features on a cadence within an agile framework Contribute to the definition of user stories Collaborate with other members of the team, including offshore, on development integration Write unit tests and maintain high code quality, per both static code analysis team standards Be available on a rotating schedule for production issues What You Need for this Position Must-Have BS in Computer Science or equivalent 5+ years of experience in software/data engineering Expertise in ETL/ELT techniques Expertise in SQL Experience working with AWS or Google Cloud Strong Unix/Linux fundamentals Nice To Have Experience with data processing frameworks e.g. Spark Experience with NoSQL column-store databases Exposure to machine learning concepts Exposure to ad tech concepts terminology What's In It for You Competitive salary with high bonus potential Collaborative and creative atmosphere, with inspired leadership Career advancement opportunities Recognition and reward for outstanding performance Great medical, dental; vision Insurance packages Competitive 401K with company match to plan for the long term Unlimited paid-time-off Transportation benefits, cell phone reimbursement Casual dress all day, every day So, if you are a Senior Data Engineer with experience, please apply today! Email Your Resume In Word To Looking forward to receiving your resume through our website and going over the position with you. Clicking apply is the best way to apply, but you may also: Shiv.Warrier@CyberCoders.com Please do NOT change the email subject line in any way. You must keep the JobID: linkedin : SW3-1634173 -- in the email subject line for your application to be considered.*** Shiv Warrier - Sr. Executive Recruiter - CyberCoders Applicants must be authorized to work in the U.S. CyberCoders, Inc is proud to be an Equal Opportunity Employer All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, protected veteran status, or any other characteristic protected by law. Your Right to Work - In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification document form upon hire.\n",
    "\"\"\"\n",
    "\n",
    "xqq = client.embeddings.create(input=query, model=EMBEDDING_MODEL).data[0].embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'description': {'vector_count': 7768},\n",
       "                'skills': {'vector_count': 7768}},\n",
       " 'total_vector_count': 15536}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_res = index.query(top_k=3, vector=xqq, namespace=\"description\", include_metadata=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'matches': [{'id': '1707',\n",
       "              'metadata': {'extras': '{\\'id\\': \\'1707\\', \\'description\\': \"Job '\n",
       "                                     'Title: Senior Data Engineer Location: '\n",
       "                                     'Alexandria, VA Salary Range: $120k - '\n",
       "                                     '$150k Requirements: ETL/ELT, SQL, '\n",
       "                                     'AWS/Google Cloud, Linux/Unix, Spark '\n",
       "                                     '(preferred), NoSQL (preferred), Machine '\n",
       "                                     'Learning concepts (preferred) Based in '\n",
       "                                     'beautiful Alexandria, VA, we are one of '\n",
       "                                     'the hottest media analytics and software '\n",
       "                                     'start-ups in the DC area. Due to growth, '\n",
       "                                     'we are actively seeking to hire a Senior '\n",
       "                                     'Data Engineer to join our team. The '\n",
       "                                     'ideal candidate will have at least 5 '\n",
       "                                     'years of experience with data pipelines '\n",
       "                                     '(built with Python), ETL experience, a '\n",
       "                                     'strong SQL background, experience with '\n",
       "                                     'AWS or Google Cloud, and strong '\n",
       "                                     'Unix/Linux fundamentals. Any experience '\n",
       "                                     'with Spark, NoSQL, and Machine Learning '\n",
       "                                     'would be a huge plus. If this sounds '\n",
       "                                     'like you, please apply now or send your '\n",
       "                                     'resume to shiv.warrier@cybercoders.com! '\n",
       "                                     'What You Will Be Doing Lead design and '\n",
       "                                     'development of data pipelines Deliver '\n",
       "                                     'features on a cadence within an agile '\n",
       "                                     'framework Contribute to the definition '\n",
       "                                     'of user stories Collaborate with other '\n",
       "                                     'members of the team, including offshore, '\n",
       "                                     'on development integration Write unit '\n",
       "                                     'tests and maintain high code quality, '\n",
       "                                     'per both static code analysis team '\n",
       "                                     'standards Be available on a rotating '\n",
       "                                     'schedule for production issues What You '\n",
       "                                     'Need for this Position Must-Have BS in '\n",
       "                                     'Computer Science or equivalent 5+ years '\n",
       "                                     'of experience in software/data '\n",
       "                                     'engineering Expertise in ETL/ELT '\n",
       "                                     'techniques Expertise in SQL Experience '\n",
       "                                     'working with AWS or Google Cloud Strong '\n",
       "                                     'Unix/Linux fundamentals Nice To Have '\n",
       "                                     'Experience with data processing '\n",
       "                                     'frameworks e.g. Spark Experience with '\n",
       "                                     'NoSQL column-store databases Exposure to '\n",
       "                                     'machine learning concepts Exposure to ad '\n",
       "                                     \"tech concepts terminology What's In It \"\n",
       "                                     'for You Competitive salary with high '\n",
       "                                     'bonus potential Collaborative and '\n",
       "                                     'creative atmosphere, with inspired '\n",
       "                                     'leadership Career advancement '\n",
       "                                     'opportunities Recognition and reward for '\n",
       "                                     'outstanding performance Great medical, '\n",
       "                                     'dental; vision Insurance packages '\n",
       "                                     'Competitive 401K with company match to '\n",
       "                                     'plan for the long term Unlimited '\n",
       "                                     'paid-time-off Transportation benefits, '\n",
       "                                     'cell phone reimbursement Casual dress '\n",
       "                                     'all day, every day So, if you are a '\n",
       "                                     'Senior Data Engineer with experience, '\n",
       "                                     'please apply today! Email Your Resume In '\n",
       "                                     'Word To Looking forward to receiving '\n",
       "                                     'your resume through our website and '\n",
       "                                     'going over the position with you. '\n",
       "                                     'Clicking apply is the best way to apply, '\n",
       "                                     'but you may also: '\n",
       "                                     'Shiv.Warrier@CyberCoders.com Please do '\n",
       "                                     'NOT change the email subject line in any '\n",
       "                                     'way. You must keep the JobID: linkedin : '\n",
       "                                     'SW3-1634173 -- in the email subject line '\n",
       "                                     'for your application to be '\n",
       "                                     'considered.*** Shiv Warrier - Sr. '\n",
       "                                     'Executive Recruiter - CyberCoders '\n",
       "                                     'Applicants must be authorized to work in '\n",
       "                                     'the U.S. CyberCoders, Inc is proud to be '\n",
       "                                     'an Equal Opportunity Employer All '\n",
       "                                     'qualified applicants will receive '\n",
       "                                     'consideration for employment without '\n",
       "                                     'regard to race, color, religion, sex, '\n",
       "                                     'national origin, disability, protected '\n",
       "                                     'veteran status, or any other '\n",
       "                                     'characteristic protected by law. Your '\n",
       "                                     'Right to Work - In compliance with '\n",
       "                                     'federal law, all persons hired will be '\n",
       "                                     'required to verify identity and '\n",
       "                                     'eligibility to work in the United States '\n",
       "                                     'and to complete the required employment '\n",
       "                                     'eligibility verification document form '\n",
       "                                     'upon hire.\", \\'industries\\': \\'Broadcast '\n",
       "                                     \"Media', 'company': 'CyberCoders', \"\n",
       "                                     \"'education': 'bachelor degree', \"\n",
       "                                     \"'months_experience': 60.0, 'sal_high': \"\n",
       "                                     \"0, 'sal_low': 0, 'salary': 0, 'title': \"\n",
       "                                     \"'Senior Data Engineer', 'skills': \"\n",
       "                                     \"'NoSQL,Agile Methodology,Unit \"\n",
       "                                     'Testing,Data Processing,Unix,Data '\n",
       "                                     'Engineering,Linux,Python (Programming '\n",
       "                                     'Language),Apache Spark,Machine '\n",
       "                                     'Learning,Software Quality (SQA/SQC),SQL '\n",
       "                                     '(Programming Language),Data '\n",
       "                                     'Pipelines,Amazon Web Services,Static '\n",
       "                                     'Program Analysis,Extract Transform Load '\n",
       "                                     '(ETL),Executive Recruitment,Computer '\n",
       "                                     'Science,Google Cloud Platform (GCP),User '\n",
       "                                     \"Story,Leadership'}\"},\n",
       "              'score': 0.982218087,\n",
       "              'values': []},\n",
       "             {'id': '0',\n",
       "              'metadata': {'extras': '{\\'id\\': \\'0\\', \\'description\\': \"Job '\n",
       "                                     'Title: Senior Data Engineer Location: '\n",
       "                                     'Alexandria, VA Salary Range: $120k - '\n",
       "                                     '$150k Requirements: ETL/ELT, SQL, '\n",
       "                                     'AWS/Google Cloud, Linux/Unix, Spark '\n",
       "                                     '(preferred), NoSQL (preferred), Machine '\n",
       "                                     'Learning concepts (preferred) Based in '\n",
       "                                     'beautiful Alexandria, VA, we are one of '\n",
       "                                     'the hottest media analytics and software '\n",
       "                                     'start-ups in the DC area. Due to growth, '\n",
       "                                     'we are actively seeking to hire a Senior '\n",
       "                                     'Data Engineer to join our team. The '\n",
       "                                     'ideal candidate will have at least 5 '\n",
       "                                     'years of experience with data pipelines '\n",
       "                                     '(built with Python), ETL experience, a '\n",
       "                                     'strong SQL background, experience with '\n",
       "                                     'AWS or Google Cloud, and strong '\n",
       "                                     'Unix/Linux fundamentals. Any experience '\n",
       "                                     'with Spark, NoSQL, and Machine Learning '\n",
       "                                     'would be a huge plus. If this sounds '\n",
       "                                     'like you, please apply now or send your '\n",
       "                                     'resume to shiv.warrier@cybercoders.com! '\n",
       "                                     'What You Will Be Doing Lead design and '\n",
       "                                     'development of data pipelines Deliver '\n",
       "                                     'features on a cadence within an agile '\n",
       "                                     'framework Contribute to the definition '\n",
       "                                     'of user stories Collaborate with other '\n",
       "                                     'members of the team, including offshore, '\n",
       "                                     'on development integration Write unit '\n",
       "                                     'tests and maintain high code quality, '\n",
       "                                     'per both static code analysis team '\n",
       "                                     'standards Be available on a rotating '\n",
       "                                     'schedule for production issues What You '\n",
       "                                     'Need for this Position Must-Have BS in '\n",
       "                                     'Computer Science or equivalent 5+ years '\n",
       "                                     'of experience in software/data '\n",
       "                                     'engineering Expertise in ETL/ELT '\n",
       "                                     'techniques Expertise in SQL Experience '\n",
       "                                     'working with AWS or Google Cloud Strong '\n",
       "                                     'Unix/Linux fundamentals Nice To Have '\n",
       "                                     'Experience with data processing '\n",
       "                                     'frameworks e.g. Spark Experience with '\n",
       "                                     'NoSQL column-store databases Exposure to '\n",
       "                                     'machine learning concepts Exposure to ad '\n",
       "                                     \"tech concepts terminology What's In It \"\n",
       "                                     'for You Competitive salary with high '\n",
       "                                     'bonus potential Collaborative and '\n",
       "                                     'creative atmosphere, with inspired '\n",
       "                                     'leadership Career advancement '\n",
       "                                     'opportunities Recognition and reward for '\n",
       "                                     'outstanding performance Great medical, '\n",
       "                                     'dental; vision Insurance packages '\n",
       "                                     'Competitive 401K with company match to '\n",
       "                                     'plan for the long term Unlimited '\n",
       "                                     'paid-time-off Transportation benefits, '\n",
       "                                     'cell phone reimbursement Casual dress '\n",
       "                                     'all day, every day So, if you are a '\n",
       "                                     'Senior Data Engineer with experience, '\n",
       "                                     'please apply today! Email Your Resume In '\n",
       "                                     'Word To Looking forward to receiving '\n",
       "                                     'your resume through our website and '\n",
       "                                     'going over the position with you. '\n",
       "                                     'Clicking apply is the best way to apply, '\n",
       "                                     'but you may also: '\n",
       "                                     'Shiv.Warrier@CyberCoders.com Please do '\n",
       "                                     'NOT change the email subject line in any '\n",
       "                                     'way. You must keep the JobID: linkedin : '\n",
       "                                     'SW3-1634173 -- in the email subject line '\n",
       "                                     'for your application to be '\n",
       "                                     'considered.*** Shiv Warrier - Sr. '\n",
       "                                     'Executive Recruiter - CyberCoders '\n",
       "                                     'Applicants must be authorized to work in '\n",
       "                                     'the U.S. CyberCoders, Inc is proud to be '\n",
       "                                     'an Equal Opportunity Employer All '\n",
       "                                     'qualified applicants will receive '\n",
       "                                     'consideration for employment without '\n",
       "                                     'regard to race, color, religion, sex, '\n",
       "                                     'national origin, disability, protected '\n",
       "                                     'veteran status, or any other '\n",
       "                                     'characteristic protected by law. Your '\n",
       "                                     'Right to Work - In compliance with '\n",
       "                                     'federal law, all persons hired will be '\n",
       "                                     'required to verify identity and '\n",
       "                                     'eligibility to work in the United States '\n",
       "                                     'and to complete the required employment '\n",
       "                                     'eligibility verification document form '\n",
       "                                     'upon hire.\", \\'industries\\': \\'Broadcast '\n",
       "                                     \"Media', 'company': 'CyberCoders', \"\n",
       "                                     \"'education': 'bachelor degree', \"\n",
       "                                     \"'months_experience': 60.0, 'sal_high': \"\n",
       "                                     \"0, 'sal_low': 0, 'salary': 0, 'title': \"\n",
       "                                     \"'Senior Data Engineer', 'skills': \"\n",
       "                                     \"'NoSQL,Agile Methodology,Unit \"\n",
       "                                     'Testing,Data Processing,Unix,Data '\n",
       "                                     'Engineering,Linux,Python (Programming '\n",
       "                                     'Language),Apache Spark,Machine '\n",
       "                                     'Learning,Software Quality (SQA/SQC),SQL '\n",
       "                                     '(Programming Language),Data '\n",
       "                                     'Pipelines,Amazon Web Services,Static '\n",
       "                                     'Program Analysis,Extract Transform Load '\n",
       "                                     '(ETL),Executive Recruitment,Computer '\n",
       "                                     'Science,Google Cloud Platform (GCP),User '\n",
       "                                     \"Story,Leadership'}\"},\n",
       "              'score': 0.982218087,\n",
       "              'values': []},\n",
       "             {'id': '1705',\n",
       "              'metadata': {'extras': '{\\'id\\': \\'1705\\', \\'description\\': \"Job '\n",
       "                                     'Title: Senior Data Engineer Location: '\n",
       "                                     'Alexandria, VA (Onsite 1-2 days/week) '\n",
       "                                     'Salary Range: $140k - $170k '\n",
       "                                     'Requirements: ETL/ELT, SQL, AWS/Google '\n",
       "                                     'Cloud, Linux/Unix, Spark (preferred), '\n",
       "                                     'NoSQL (preferred), Machine Learning '\n",
       "                                     'concepts (preferred) Based in beautiful '\n",
       "                                     'Alexandria, VA, we are one of the '\n",
       "                                     'hottest media analytics and software '\n",
       "                                     'start-ups in the DC area. Due to growth, '\n",
       "                                     'we are actively seeking to hire a Senior '\n",
       "                                     'Data Engineer to join our team. The '\n",
       "                                     'ideal candidate will have at least 5 '\n",
       "                                     'years of experience with data pipelines '\n",
       "                                     '(built with Python), ETL experience, a '\n",
       "                                     'strong SQL background, experience with '\n",
       "                                     'AWS or Google Cloud, and strong '\n",
       "                                     'Unix/Linux fundamentals. Any experience '\n",
       "                                     'with Spark, NoSQL, and Machine Learning '\n",
       "                                     'would be a huge plus. If this sounds '\n",
       "                                     'like you, please apply now or send your '\n",
       "                                     'resume to shiv.warrier@cybercoders.com! '\n",
       "                                     'What You Will Be Doing Lead design and '\n",
       "                                     'development of data pipelines Deliver '\n",
       "                                     'features on a cadence within an agile '\n",
       "                                     'framework Contribute to the definition '\n",
       "                                     'of user stories Collaborate with other '\n",
       "                                     'members of the team, including offshore, '\n",
       "                                     'on development integration Write unit '\n",
       "                                     'tests and maintain high code quality, '\n",
       "                                     'per both static code analysis team '\n",
       "                                     'standards Be available on a rotating '\n",
       "                                     'schedule for production issues What You '\n",
       "                                     'Need for this Position Must-Have BS in '\n",
       "                                     'Computer Science or equivalent 5+ years '\n",
       "                                     'of experience in software/data '\n",
       "                                     'engineering Expertise in ETL/ELT '\n",
       "                                     'techniques Expertise in SQL Experience '\n",
       "                                     'working with AWS or Google Cloud Strong '\n",
       "                                     'Unix/Linux fundamentals Nice To Have '\n",
       "                                     'Experience with data processing '\n",
       "                                     'frameworks e.g. Spark Experience with '\n",
       "                                     'NoSQL column-store databases Exposure to '\n",
       "                                     'machine learning concepts Exposure to ad '\n",
       "                                     \"tech concepts terminology What's In It \"\n",
       "                                     'for You Competitive salary with high '\n",
       "                                     'bonus potential Collaborative and '\n",
       "                                     'creative atmosphere, with inspired '\n",
       "                                     'leadership Career advancement '\n",
       "                                     'opportunities Recognition and reward for '\n",
       "                                     'outstanding performance Great medical, '\n",
       "                                     'dental; vision Insurance packages '\n",
       "                                     'Competitive 401K with company match to '\n",
       "                                     'plan for the long term Unlimited '\n",
       "                                     'paid-time-off Transportation benefits, '\n",
       "                                     'cell phone reimbursement Casual dress '\n",
       "                                     'all day, every day So, if you are a '\n",
       "                                     'Senior Data Engineer with experience, '\n",
       "                                     'please apply today! Email Your Resume In '\n",
       "                                     'Word To Looking forward to receiving '\n",
       "                                     'your resume through our website and '\n",
       "                                     'going over the position with you. '\n",
       "                                     'Clicking apply is the best way to apply, '\n",
       "                                     'but you may also: '\n",
       "                                     'Shiv.Warrier@CyberCoders.com Please do '\n",
       "                                     'NOT change the email subject line in any '\n",
       "                                     'way. You must keep the JobID: linkedin : '\n",
       "                                     'SW3-1639508 -- in the email subject line '\n",
       "                                     'for your application to be '\n",
       "                                     'considered.*** Shiv Warrier - Sr. '\n",
       "                                     'Executive Recruiter - CyberCoders '\n",
       "                                     'Applicants must be authorized to work in '\n",
       "                                     'the U.S. CyberCoders, Inc is proud to be '\n",
       "                                     'an Equal Opportunity Employer All '\n",
       "                                     'qualified applicants will receive '\n",
       "                                     'consideration for employment without '\n",
       "                                     'regard to race, color, religion, sex, '\n",
       "                                     'national origin, disability, protected '\n",
       "                                     'veteran status, or any other '\n",
       "                                     'characteristic protected by law. Your '\n",
       "                                     'Right to Work - In compliance with '\n",
       "                                     'federal law, all persons hired will be '\n",
       "                                     'required to verify identity and '\n",
       "                                     'eligibility to work in the United States '\n",
       "                                     'and to complete the required employment '\n",
       "                                     'eligibility verification document form '\n",
       "                                     'upon hire.\", \\'industries\\': \\'Broadcast '\n",
       "                                     \"Media', 'company': 'CyberCoders', \"\n",
       "                                     \"'education': 'bachelor degree', \"\n",
       "                                     \"'months_experience': 60.0, 'sal_high': \"\n",
       "                                     \"0, 'sal_low': 0, 'salary': 0, 'title': \"\n",
       "                                     \"'Senior Data Engineer', 'skills': \"\n",
       "                                     \"'NoSQL,Agile Methodology,Unit \"\n",
       "                                     'Testing,Data Processing,Unix,Data '\n",
       "                                     'Engineering,Linux,Python (Programming '\n",
       "                                     'Language),Apache Spark,Machine '\n",
       "                                     'Learning,Software Quality (SQA/SQC),SQL '\n",
       "                                     '(Programming Language),Data '\n",
       "                                     'Pipelines,Amazon Web Services,Static '\n",
       "                                     'Program Analysis,Extract Transform Load '\n",
       "                                     '(ETL),Executive Recruitment,Computer '\n",
       "                                     'Science,Google Cloud Platform (GCP),User '\n",
       "                                     \"Story,Leadership'}\"},\n",
       "              'score': 0.977007508,\n",
       "              'values': []}],\n",
       " 'namespace': 'description',\n",
       " 'usage': {'read_units': 6}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for i in match_res.matches:\n",
    "    a = {}\n",
    "    metadata = i['metadata']\n",
    "    res.append(eval(metadata['extras'])['skills'].split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['NoSQL',\n",
       "  'Agile Methodology',\n",
       "  'Unit Testing',\n",
       "  'Data Processing',\n",
       "  'Unix',\n",
       "  'Data Engineering',\n",
       "  'Linux',\n",
       "  'Python (Programming Language)',\n",
       "  'Apache Spark',\n",
       "  'Machine Learning',\n",
       "  'Software Quality (SQA/SQC)',\n",
       "  'SQL (Programming Language)',\n",
       "  'Data Pipelines',\n",
       "  'Amazon Web Services',\n",
       "  'Static Program Analysis',\n",
       "  'Extract Transform Load (ETL)',\n",
       "  'Executive Recruitment',\n",
       "  'Computer Science',\n",
       "  'Google Cloud Platform (GCP)',\n",
       "  'User Story',\n",
       "  'Leadership'],\n",
       " ['NoSQL',\n",
       "  'Agile Methodology',\n",
       "  'Unit Testing',\n",
       "  'Data Processing',\n",
       "  'Unix',\n",
       "  'Data Engineering',\n",
       "  'Linux',\n",
       "  'Python (Programming Language)',\n",
       "  'Apache Spark',\n",
       "  'Machine Learning',\n",
       "  'Software Quality (SQA/SQC)',\n",
       "  'SQL (Programming Language)',\n",
       "  'Data Pipelines',\n",
       "  'Amazon Web Services',\n",
       "  'Static Program Analysis',\n",
       "  'Extract Transform Load (ETL)',\n",
       "  'Executive Recruitment',\n",
       "  'Computer Science',\n",
       "  'Google Cloud Platform (GCP)',\n",
       "  'User Story',\n",
       "  'Leadership'],\n",
       " ['NoSQL',\n",
       "  'Agile Methodology',\n",
       "  'Unit Testing',\n",
       "  'Data Processing',\n",
       "  'Unix',\n",
       "  'Data Engineering',\n",
       "  'Linux',\n",
       "  'Python (Programming Language)',\n",
       "  'Apache Spark',\n",
       "  'Machine Learning',\n",
       "  'Software Quality (SQA/SQC)',\n",
       "  'SQL (Programming Language)',\n",
       "  'Data Pipelines',\n",
       "  'Amazon Web Services',\n",
       "  'Static Program Analysis',\n",
       "  'Extract Transform Load (ETL)',\n",
       "  'Executive Recruitment',\n",
       "  'Computer Science',\n",
       "  'Google Cloud Platform (GCP)',\n",
       "  'User Story',\n",
       "  'Leadership']]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Extract Transform Load (ETL)', 'Executive Recruitment', 'Google Cloud Platform (GCP)', 'Data Processing', 'Unix', 'NoSQL', 'Leadership', 'Machine Learning', 'Apache Spark', 'Linux', 'Computer Science', 'SQL (Programming Language)', 'Amazon Web Services', 'Python (Programming Language)', 'Data Engineering', 'Static Program Analysis', 'Software Quality (SQA/SQC)', 'Agile Methodology', 'Unit Testing', 'User Story', 'Data Pipelines'}\n"
     ]
    }
   ],
   "source": [
    "common_skills = set(res[0])\n",
    "\n",
    "# Iterate over the rest of the lists and find common elements\n",
    "for skills in res[1:]:\n",
    "    common_skills = common_skills.intersection(set(skills))\n",
    "\n",
    "print(common_skills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Extract Transform Load (ETL)', 'Executive Recruitment', 'Google Cloud Platform (GCP)', 'Data Processing', 'Unix', 'NoSQL', 'Leadership', 'Machine Learning', 'Apache Spark', 'Linux', 'Computer Science', 'SQL (Programming Language)', 'Amazon Web Services', 'Python (Programming Language)', 'Data Engineering', 'Static Program Analysis', 'Software Quality (SQA/SQC)', 'Agile Methodology', 'Unit Testing', 'User Story', 'Data Pipelines'}\n"
     ]
    }
   ],
   "source": [
    "common_skills = set(res[0])\n",
    "\n",
    "# Iterate over the rest of the lists and find common elements\n",
    "for skills in res[1:]:\n",
    "    common_skills = common_skills.union(set(skills))\n",
    "\n",
    "print(common_skills)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
